{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMakerCore Inference, Async Inference, and Resource Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductions\n",
    "\n",
    "In this notebook, we will walkthrough how to perform Sync and Async Inference using SageMakerCore. Additionaly, this notebook will highlight how to create an endpoint using the Resource Chaining feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Chaining\n",
    "\n",
    "Resource Chaining is a feature provided by SageMakerCore that aims to reduce the cognitive load when performing operation with SageMakerCore. The idea is to allow users to create an object, for example a  `Model` resource object, and pass the object directly as a parameter to some other resource like `EndpointConfig`. An example of this chaining can be seen below:\n",
    "\n",
    "```python\n",
    "key = f'xgboost-iris-{strftime(\"%H-%M-%S\", gmtime())}'\n",
    "\n",
    "model = Model.create(...) # Create model object\n",
    "\n",
    "endpoint_config = ndpointConfig.create(\n",
    "    endpoint_config_name=key,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=key,\n",
    "            initial_instance_count=1,\n",
    "            instance_type='ml.m5.xlarge',\n",
    "            model_name=model # Pass model object directly\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Packages\n",
    "Ensure you are using a kernel with python version >=3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sagemaker_core - replace below with path to local tar.gz file\n",
    "\n",
    "!pip install <path to sagemaker_core tar.gz file>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additionall packages\n",
    "\n",
    "!pip install -U sagemaker scikit-learn pandas boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "- AWS region.\n",
    "- The IAM role arn used to give learning and hosting access to your data. Ensure your enviornment has AWS Credentials configured.\n",
    "- The S3 bucket that you want to use for storing training and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "# Get region, role, bucket\n",
    "\n",
    "sagemaker_session = Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the XGBoost Image URI\n",
    "In this step, we will fetch the XGBoost Image URI we will use as an input parameter when creating an AWS TrainingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Fetch XGBOOST image\n",
    "\n",
    "image = image_uris.retrieve(framework='xgboost', region=region, version=\"latest\")\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Model Data to S3\n",
    "In this step, we will upload the model data to the S3 bucket configured earlier using `sagemaker_session.default_bucket()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Upload Data\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "DATA_DIRECTORY = \"data\"\n",
    "prefix = \"DEMO-scikit-iris-inference\"\n",
    "XGBOOST_MODEL = \"xgboost-model.tar.gz\"\n",
    "\n",
    "model_data_uri = sagemaker_session.upload_data(os.path.join(DATA_DIRECTORY, XGBOOST_MODEL), bucket, prefix + \"/model\")\n",
    "print(model_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint Using Resource Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.generated.shapes import ContainerDefinition, ProductionVariant\n",
    "from sagemaker_core.generated.resources import Model, EndpointConfig, Endpoint\n",
    "from time import gmtime, strftime\n",
    "\n",
    "MODEL_PATH = \"data/iris_model.tar.gz\"\n",
    "\n",
    "key = f'xgboost-iris-{strftime(\"%H-%M-%S\", gmtime())}'\n",
    "print(\"key\", key)\n",
    "\n",
    "model = Model.create(\n",
    "    model_name=key,\n",
    "    primary_container=ContainerDefinition(\n",
    "        image=image,\n",
    "        model_data_url=model_data_uri,\n",
    "        # here we are getting model data from the training job \n",
    "        environment={\n",
    "            'LOCAL_PYTHON': '3.10.12',\n",
    "            'MODEL_CLASS_NAME': 'xgboost.sklearn.XGBClassifier',\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': '10',\n",
    "            'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "            'SAGEMAKER_REGION': 'us-west-2',\n",
    "            'SAGEMAKER_SERVE_SECRET_KEY': '3a459322560a181436866602ddfbb7c16ea97046e92845de43a5ac80f7604451',\n",
    "            'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'\n",
    "        }\n",
    "    ),\n",
    "    execution_role_arn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=key,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=key,\n",
    "            initial_instance_count=1,\n",
    "            instance_type='ml.m5.xlarge',\n",
    "            model_name=model # Pass model object created above\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "endpoint: Endpoint = Endpoint.create(\n",
    "    endpoint_name=key,\n",
    "    endpoint_config_name=endpoint_config # Pass endpoint config object created above\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.wait_for_status(\"InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.base_serializers import NumpySerializer, CSVSerializer\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "dataset = loadtxt('data/pima-indians-diabetes.data.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "serializer = NumpySerializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get IRIS Data\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "\n",
    "import os\n",
    "\n",
    "# Prepare Data\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "iris_df = iris_df[['target'] + [col for col in iris_df.columns if col != 'target']]\n",
    "\n",
    "train_data, test_data = train_test_split(iris_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data.to_csv('./data/train.csv', index=False, header=False)\n",
    "test_data.to_csv('./data/test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint Invoke Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialise(response):\n",
    "    return np.load(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "\n",
    "invoke_result = endpoint.invoke(body=serializer.serialize(test_data),\n",
    "                                content_type='text/csv',\n",
    "                                accept='text/csv')\n",
    "\n",
    "print(\"Endpoint Response:\", deserialise(invoke_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint Invoke With Response Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialise(response):\n",
    "    return [\n",
    "        res_part\n",
    "        for res_part in response['Body']\n",
    "    ]\n",
    "\n",
    "\n",
    "invoke_result = endpoint.invoke_with_response_stream(body=serializer.serialize(X_test),\n",
    "                                                     content_type='application/x-npy',\n",
    "                                                     accept='application/x-npy')\n",
    "\n",
    "print(\"Endpoint Response:\", deserialise(invoke_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint Invoke Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.generated.shapes import ProductionVariant, AsyncInferenceConfig, AsyncInferenceOutputConfig, AsyncInferenceClientConfig\n",
    "\n",
    "\n",
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=key,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=\"variant1\",\n",
    "            model_name=model,\n",
    "            instance_type='ml.m5.xlarge',\n",
    "            initial_instance_count=1\n",
    "        )\n",
    "    ],\n",
    "    async_inference_config=AsyncInferenceConfig(\n",
    "        output_config=AsyncInferenceOutputConfig(s3_output_path=f\"s3://{bucket}/{prefix}/output\"),\n",
    "        client_config=AsyncInferenceClientConfig(\n",
    "            max_concurrent_invocations_per_instance=4\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "endpoint = Endpoint.create(endpoint_name=key, endpoint_config_name=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(input_location):\n",
    "    prefix = f\"{prefix}/input\"\n",
    "    return sagemaker_session.upload_data(\n",
    "        input_location,\n",
    "        bucket=sagemaker_session.default_bucket(),\n",
    "        key_prefix=prefix,\n",
    "        extra_args={\"ContentType\": \"text/libsvm\"},\n",
    "    )\n",
    "\n",
    "input_1_location = \"input/test_point_0.libsvm\"\n",
    "input_1_s3_location = upload_file(input_1_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoint.invoke_async(input_location=input_1_s3_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
